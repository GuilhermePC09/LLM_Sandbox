{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from threading import Thread\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/nj67kb651dlb556srctrpq800000gn/T/ipykernel_10228/2766369429.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(\n"
     ]
    }
   ],
   "source": [
    "model = Ollama(\n",
    "        model=\"llama3\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        verbose=True,\n",
    "        temperature=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    global qa_chain  \n",
    "    if qa_chain is None:\n",
    "        return jsonify({'error': 'No QA chain initialized'}), 400\n",
    "    \n",
    "    try:\n",
    "        user_input = request.json.get('question')\n",
    "        if not user_input:\n",
    "            return jsonify({'error': 'Question is missing in the request'}), 400\n",
    "\n",
    "        else:\n",
    "            response = qa_chain.invoke({\n",
    "                \"question\": user_input,\n",
    "                \"chat_history\": []\n",
    "            })\n",
    "        \n",
    "        serializable_response = {\n",
    "            'answer': response.get('answer', ''),\n",
    "            'source_documents': [\n",
    "                {\n",
    "                    'source': doc.metadata.get('source', ''),\n",
    "                    'page': doc.metadata.get('page', ''),\n",
    "                    'content': doc.page_content\n",
    "                }\n",
    "                for doc in response.get('source_documents', [])\n",
    "            ],\n",
    "            'generated_question': response.get('generated_question', '')\n",
    "        }\n",
    "\n",
    "        return jsonify(serializable_response), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/load_pdf', methods=['POST'])\n",
    "def load_pdf():\n",
    "    global qa_chain \n",
    "    file = request.files['file']\n",
    "    if file:\n",
    "        file_path = os.path.join('docs', file.filename)\n",
    "        file.save(file_path)\n",
    "        \n",
    "        qa_chain = load_db(file=file_path, chain_type='stuff', k=3)\n",
    "        \n",
    "        return jsonify({'message': 'PDF loaded successfully'}), 200\n",
    "    return jsonify({'error': 'No file provided'}), 400\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(\"\"\"\n",
    "        <h1>Welcome to the Chat API</h1>\n",
    "        <p>Send a POST request to /ask to ask questions.</p>\n",
    "        <form id=\"ask-form\">\n",
    "            <label for=\"question\">Write Your Question:</label>\n",
    "            <input type=\"text\" id=\"question\" name=\"question\" required>\n",
    "            <button type=\"submit\">Ask</button>\n",
    "        </form>\n",
    "        <form action=\"/load_pdf\" method=\"POST\" enctype=\"multipart/form-data\">\n",
    "            <label for=\"file\">Choose a PDF to load:</label>\n",
    "            <input type=\"file\" name=\"file\" accept=\".pdf\" required>\n",
    "            <button type=\"submit\">Upload PDF</button>\n",
    "        </form>\n",
    "\n",
    "        <script>\n",
    "            document.getElementById('ask-form').onsubmit = async function(event) {\n",
    "                event.preventDefault();\n",
    "                \n",
    "                const question = document.getElementById('question').value;\n",
    "\n",
    "                const response = await fetch('/ask', {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify({ question: question })\n",
    "                });\n",
    "\n",
    "                const data = await response.json();\n",
    "                if (data.error) {\n",
    "                    alert(\"Error: \" + data.error);\n",
    "                } else {\n",
    "                    alert(\"Response: \" + data.response);\n",
    "                }\n",
    "            };\n",
    "        </script>\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def run_flask():\n",
    "    app.run(debug=True, use_reloader=False, host='localhost', port=5000)\n",
    "\n",
    "def start_flask_thread():\n",
    "    flask_thread = Thread(target=run_flask)\n",
    "    flask_thread.start()\n",
    "    return flask_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://localhost:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "/Users/guilhermepifferchristo/Desktop/LLM_sandbox/venv/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n",
      "127.0.0.1 - - [16/Dec/2024 13:37:25] \"POST /load_pdf HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Dec/2024 13:38:02] \"POST /ask HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Dec/2024 13:38:52] \"POST /ask HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Dec/2024 13:41:36] \"POST /load_pdf HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Dec/2024 13:41:58] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "flask_thread = start_flask_thread()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
